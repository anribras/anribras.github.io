---
layout: post
title:
modified:
categories: Tech
 
tags: [web,zeromq]

  
comments: true
---

<!-- TOC -->

- [场景](#场景)
- [单点(Cluster)实现](#单点cluster实现)
- [Multiple Clusters](#multiple-clusters)
    - [worker 的REQ变成ROUTER](#worker-的req变成router)
    - [broker 之间](#broker-之间)
        - [state flow](#state-flow)
        - [cloud and local flow](#cloud-and-local-flow)
- [缺陷](#缺陷)
- [灵感一下](#灵感一下)
- [其他例子](#其他例子)

<!-- /TOC -->

## 场景

n个集群构成的云，每个单点(A cluster)计算能力不一，都运行了clients和workers.

clients可同时向workers发送task,若单点本身无可用worker,向其他空闲单点发送task. worker处理完毕后，向clients发送reply;

单点随时可能移除(crash)和上线(add),弹性伸缩能力!

## 单点(Cluster)实现

单点就用前面提到的load balancing broker:

![2018-09-14-14-26-50](https://images-1257933000.cos.ap-chengdu.myqcloud.com/2018-09-14-14-26-50.png)

这个single cluster的问题是worker太慢,(pop 前可以看到workers list里的worker越来越少，最后肯定是client 等待worker).

如果有multiple cluster,把当前broker处理不过来的client交给其他broker的worker处理，岂不美哉?

## Multiple Clusters

![2018-09-19-09-42-26](https://images-1257933000.cos.ap-chengdu.myqcloud.com/2018-09-19-09-42-26.png)

多点怎么办？client需要和另外的broker通信；workers需要告诉其他broker"我有空",broker之间也需要交流.

### worker 的REQ变成ROUTER
原模型的REQ 只能通知１个ROUTER(proxy), 将worker改为ROUTER后，ROUTER->ROUTER.可以routing的方式通知多个broker;

![2018-09-19-09-43-04](https://images-1257933000.cos.ap-chengdu.myqcloud.com/2018-09-19-09-43-04.png)

### broker 之间

borker之间可用额外的network type保持沟通.甚至增加broker center来与所有broker交互.

![2018-09-19-09-53-42](https://images-1257933000.cos.ap-chengdu.myqcloud.com/2018-09-19-09-53-42.png)

假设n个broker，每个broker有n-1个peer,看下broker之间可能发生的交流:

* PUB-SUB, 告诉or接收peer broker可用的worker数量和状态;
* 1 ROUTER for tasks it receives  
* 1 ROUTER for tasks it delegates(委托给其他broker)

所有３个type ，双向，一共有６个方向的数据流.

结合load balancing broker内部的数据流,1个broker有３大类flows:
* A local request-reply flow between the broker and its clients and workers.(正常的load balancing flow)
* A cloud request-reply flow between the broker and its peer brokers.(broker提交task到其他broker)
* A state flow between the broker and its peer brokers.(告诉其他broker有多少空闲workers)

完整的broker构成:
![2018-09-19-10-30-01](https://images-1257933000.cos.ap-chengdu.myqcloud.com/2018-09-19-10-30-01.png)

#### state flow

PUB bind, SUB connect

PUB给对方什么？peer+当前还有多少worker,worker数量发生变化时就pub

#### cloud and local flow

local worker Reply时,local_cap +=1;
cloud worker 则是通过上面的state flow将[peer,cap],如果有多个,将cap用list保存下.

当local_cap+cloud_cap不为０时，说明worker ready,开始frontend的poll,同样可能来自local or cloud;

这里有个策略问题，client是发给cloud还是local的worker处理?　自然的想法是先local处理了，local处理不过来，再去找cloud.具体多个cloud怎么找？例子用的random，理论上应该看谁的cap最大，自然最稳妥了.
```py
        while local_capacity + cloud_capacity:
            secondary = zmq.Poller()
            secondary.register(localfe, zmq.POLLIN)
            if local_capacity:
                secondary.register(cloudfe, zmq.POLLIN)
            events = dict(secondary.poll(0))

            # We'll do peer brokers first, to prevent starvation
            if cloudfe in events:
                msg = cloudfe.recv_multipart()
            elif localfe in events:
                msg = localfe.recv_multipart()
            else:
                break  # No work, go back to backends

            if local_capacity:
                msg = [workers.pop(0), b''] + msg
                localbe.send_multipart(msg)
                local_capacity -= 1
            else:
                # Route to random broker peer
                msg = [random.choice(peers), b''] + msg
                cloudbe.send_multipart(msg)
        if local_capacity != previous:
            statebe.send_multipart([myself, asbytes(local_capacity)])
```

完整的例子:
```py
import random
import sys
import threading
import time

import zmq

NBR_CLIENTS = 100
NBR_WORKERS = 8

def asbytes(obj):
    s = str(obj)
    if str is not bytes:
        # Python 3
        s = s.encode('ascii')
    return s

def client_task(name, i):
    """Request-reply client using REQ socket"""
    ctx = zmq.Context()
    client = ctx.socket(zmq.REQ)
    client.identity = (u"Client-%s-%s" % (name, i)).encode('ascii')
    client.connect("ipc://%s-localfe.ipc" % name)
    monitor = ctx.socket(zmq.PUSH)
    monitor.connect("ipc://%s-monitor.ipc" % name)

    poller = zmq.Poller()
    poller.register(client, zmq.POLLIN)
    while True:
        time.sleep(random.randint(0, 5))
        for _ in range(random.randint(0, 15)):
            # send request with random hex ID
            task_id = u"%04X" % random.randint(0, 10000)
            client.send_string(task_id)

            # wait max 10 seconds for a reply, then complain
            try:
                events = dict(poller.poll(10000))
            except zmq.ZMQError:
                return # interrupted

            if events:
                reply = client.recv_string()
                print("expected %s, got %s" % (task_id, reply))
                assert reply == task_id, "expected %s, got %s" % (task_id, reply)
                monitor.send_string(reply)
            else:
                monitor.send_string(u"E: CLIENT EXIT - lost task %s" % task_id)
                return

def worker_task(name, i):
    """Worker using REQ socket to do LRU routing"""
    ctx = zmq.Context()
    worker = ctx.socket(zmq.REQ)
    worker.identity = ("Worker-%s-%s" % (name, i)).encode('ascii')
    worker.connect("ipc://%s-localbe.ipc" % name)

    # Tell broker we're ready for work
    worker.send(b"READY")

    # Process messages as they arrive
    while True:
        try:
            msg = worker.recv_multipart()
        except zmq.ZMQError:
            # interrupted
            return
        # Workers are busy for 0/1 seconds
        time.sleep(random.randint(0, 1))
        worker.send_multipart(msg)

def main(myself, peers):
    print("I: preparing broker at %s…" % myself)

    # Prepare our context and sockets
    ctx = zmq.Context()

    # Bind cloud frontend to endpoint
    cloudfe = ctx.socket(zmq.ROUTER)
    cloudfe.setsockopt(zmq.IDENTITY, myself)
    cloudfe.bind("ipc://%s-cloud.ipc" % myself)

    # Bind state backend / publisher to endpoint
    statebe = ctx.socket(zmq.PUB)
    statebe.bind("ipc://%s-state.ipc" % myself)

    # Connect cloud and state backends to all peers
    cloudbe = ctx.socket(zmq.ROUTER)
    statefe = ctx.socket(zmq.SUB)
    statefe.setsockopt(zmq.SUBSCRIBE, b"")
    cloudbe.setsockopt(zmq.IDENTITY, myself)

    for peer in peers:
        print("I: connecting to cloud frontend at %s" % peer)
        cloudbe.connect("ipc://%s-cloud.ipc" % peer)
        print("I: connecting to state backend at %s" % peer)
        statefe.connect("ipc://%s-state.ipc" % peer)

    # Prepare local frontend and backend
    localfe = ctx.socket(zmq.ROUTER)
    localfe.bind("ipc://%s-localfe.ipc" % myself)
    localbe = ctx.socket(zmq.ROUTER)
    localbe.bind("ipc://%s-localbe.ipc" % myself)

    # Prepare monitor socket
    monitor = ctx.socket(zmq.PULL)
    monitor.bind("ipc://%s-monitor.ipc" % myself)

    # Get user to tell us when we can start…
    # raw_input("Press Enter when all brokers are started: ")

    # create workers and clients threads
    for i in range(NBR_WORKERS):
        thread = threading.Thread(target=worker_task, args=(myself, i))
        thread.daemon = True
        thread.start()

    for i in range(NBR_CLIENTS):
        thread_c = threading.Thread(target=client_task, args=(myself, i))
        thread_c.daemon = True
        thread_c.start()

    # Interesting part
    # -------------------------------------------------------------
    # Publish-subscribe flow
    # - Poll statefe and process capacity updates
    # - Each time capacity changes, broadcast new value
    # Request-reply flow
    # - Poll primary and process local/cloud replies
    # - While worker available, route localfe to local or cloud

    local_capacity = 0
    cloud_capacity = 0
    workers = []

    # setup backend poller
    pollerbe = zmq.Poller()
    pollerbe.register(localbe, zmq.POLLIN)
    pollerbe.register(cloudbe, zmq.POLLIN)
    pollerbe.register(statefe, zmq.POLLIN)
    pollerbe.register(monitor, zmq.POLLIN)

    while True:
        # If we have no workers anyhow, wait indefinitely
        try:
            events = dict(pollerbe.poll(1000 if local_capacity else None))
        except zmq.ZMQError:
            break  # interrupted

        previous = local_capacity
        # Handle reply from local worker
        msg = None
        if localbe in events:
            msg = localbe.recv_multipart()
            (address, empty), msg = msg[:2], msg[2:]

            workers.append(address)
            local_capacity += 1

            # If it's READY, don't route the message any further
            if msg[-1] == b'READY':
                msg = None
        elif cloudbe in events:
            msg = cloudbe.recv_multipart()
            (address, empty), msg = msg[:2], msg[2:]

            # We don't use peer broker address for anything

        if msg is not None:
            address = msg[0]
            if address in peers:
                # Route reply to cloud if it's addressed to a broker
                cloudfe.send_multipart(msg)
            else:
                # Route reply to client if we still need to
                localfe.send_multipart(msg)

        # Handle capacity updates
        if statefe in events:
            peer, s = statefe.recv_multipart()
            cloud_capacity = int(s)
            print('recv cloud worker from peer %s cap %d'%(peer,cloud_capacity))

        # handle monitor message
        if monitor in events:
            print(monitor.recv_string())

        # Now route as many clients requests as we can handle
        # - If we have local capacity we poll both localfe and cloudfe
        # - If we have cloud capacity only, we poll just localfe
        # - Route any request locally if we can, else to cloud
        while local_capacity + cloud_capacity:
            print('Local %d Cloud %d' %(local_capacity, cloud_capacity))
            secondary = zmq.Poller()
            secondary.register(localfe, zmq.POLLIN)
            if local_capacity:
                secondary.register(cloudfe, zmq.POLLIN)
            events = dict(secondary.poll(0))

            # We'll do peer brokers first, to prevent starvation
            if cloudfe in events:
                msg = cloudfe.recv_multipart()
            elif localfe in events:
                msg = localfe.recv_multipart()
            else:
                break  # No work, go back to backends

            if local_capacity:
                msg = [workers.pop(0), b''] + msg
                localbe.send_multipart(msg)
                local_capacity -= 1
            else:
                # Route to random broker peer
                msg = [random.choice(peers), b''] + msg
                cloudbe.send_multipart(msg)
        if local_capacity != previous:
            print("Pub to cloud peer %d left", local_capacity)
            statebe.send_multipart([myself, asbytes(local_capacity)])

if __name__ == '__main__':
    if len(sys.argv) >= 2:
        myself = asbytes(sys.argv[1])
        main(myself, peers=[ asbytes(a) for a in sys.argv[2:] ])
    else:
        print("Usage: peering3.py 1 2 3 4")
        sys.exit(1)
```

特别注意ROUTER的使用:

>send to router时,消息第１个为address(route到那里去?)

>router received的消息第１个也一定是address(指明哪里route过来的?)

> 如果来自REQ address后面肯定有delimeter 

## 缺陷

multibroker都是直连的,增加broker时，拓扑太复杂了。显然设计１个ceter borker所有的broker跟它通信，由它统一调度的思路更清晰。

另外１个缺陷,即load balancing实现时,local worker都是注册到了１个list中，poll检出１个worker，马上就用掉了，意味着其他worker在排队等待中，这导致worker list中最多１个worker在等待，效率不是太高.而且由于数量不变，并不会通过state flow通知其他cloud broker，导致例子的演示效果不好.

## 灵感一下

看到了redis cluster的架构，是不是觉得就在眼前？

![2018-09-20-17-53-44](https://images-1257933000.cos.ap-chengdu.myqcloud.com/2018-09-20-17-53-44.png)

## 其他例子

client 访问 node server, node server从另外的python ml处理拿预测结果, node.js <--zeromq--> python.
(https://medium.com/@amit.kulkarni/consuming-services-for-web-using-zeromq-5d9817fec2cd)