---
layout: post
title:
modified:
categories: Tech
 
tags: [machine-learning]

  
comments: true
---
<!-- TOC -->

- [k-means](#k-means)
    - [流程](#流程)
    - [技巧](#技巧)
- [demensionality reduction(维度约减)](#demensionality-reduction维度约减)
    - [PCA主成分分析](#pca主成分分析)
    - [PCA使用注意](#pca使用注意)
    - [数据可视化 data visiualazation](#数据可视化-data-visiualazation)
- [一些数学知识补充](#一些数学知识补充)

<!-- /TOC -->

### k-means
clustering 算法，无监督.

#### 流程

1. 选择K
2. 随机选择k个中心 
3. 计算所有样本到每个中心的距离，距离最短的，将样本标记为第k类
4. 计算所有标记类的距离均值,作为新的中心点，步骤3.直到中心点收敛。

![2018-04-27-14-43-24](https://images-1257933000.cos.ap-chengdu.myqcloud.com/2018-04-27-14-43-24.png) 

#### 技巧

随机初始化中心点时，不是真的随机选点，而是直接从样本里选，效果更好.

某次运算后，可能陷入`局部最优`,解决方法是多来几次,选择最小的(用k-means的代价函数(失真函数distortion functiion))那次分类

$$J(c^{(1)},c^{(2)}...c^{(m)},\mu_{1},...\mu_{k}) = \sum_{i=1}^{m}||x_{c^{i}}-\mu_{k}||^{2}$$

如何选择聚类的个数K?`elbow methed`,同样看distortion functiion.也不能完全保证.另外的方法是看实际效果.

![2018-04-27-15-37-39](https://images-1257933000.cos.ap-chengdu.myqcloud.com/2018-04-27-15-37-39.png)

### demensionality reduction(维度约减)

#### PCA主成分分析

从n维降到k维,核心就是`奇异值分解`,k维到n维也可以用Ureduce(nxk x kx1)来复原。

![2018-04-28-21-38-16](https://images-1257933000.cos.ap-chengdu.myqcloud.com/2018-04-28-21-38-16.png)


选择k的一个方法:

![2018-04-29-21-47-40](https://images-1257933000.cos.ap-chengdu.myqcloud.com/2018-04-29-21-47-40.png)


问题什么是$x_{approx}$?还是不求了，直接用奇异值:

![2018-04-29-22-08-03](https://images-1257933000.cos.ap-chengdu.myqcloud.com/2018-04-29-22-08-03.png)

即降维后，数据与原始数据的均方误差，应该只能占原始数据的很少比例,1%/5%/10%

疑问是，这里都是用基本协方差矩阵，而在图像压缩里，是直接将图像矩阵svd?图像压缩把每一行看成一个特征集合(100x100,每行100)，而图像识别这类，是把整个图像作为一个特征集(100x100,特征集是10000x1).

#### PCA使用注意

PCA用来降维压缩存储/可视化比较好.

不应该降维后去做ml处理，因为实际丢失了信息，并不一定利于ml的学习，一定先用原始数据，如果发现算法特别慢，内存不够，可以考虑用PCA后评估,但仍然不推荐用PCA去降维度training data.


#### 数据可视化 data visiualazation

降维度到2-3维，再画出来

### 一些数学知识补充

* 线性相关，线性无关，秩

[参考](https://www.zhihu.com/question/39326459/answer/124420523)

[参考](https://zhuanlan.zhihu.com/p/33001392)

n个向量的组(矩阵)线性无关，表示可以由这n个向量张成一个n维空间，而线性相关则是n个向量里，可能只有m个可能张成一个m维空间，其中有的向量可以用其他向量的线性组合来表达，m<n.m即是秩

把秩比喻成一组向量“干货”的多少，一组向量线性相关表示有一个向量能被其他向量的线性组合表示出来，说明它是“水分”.

对于向量空间，这一组向量为基向量，该空间的向量都可以用一组基向量的线性组合来表达，但是基向量并不唯一.

* 向量的内积

$$u^{T}v=||v||*||u||cos(\theta)=u_{1}*v_{1}+u{2}*v_{2}$$

表示向量u在v上的投影$||u||*cos$
与
$||v||$的乘积,如果矩阵化，即:

$$y=U*v$$

表示v在一组向量矩阵上的投影，该向量可以为规范正交基,U为投影矩阵.

* 正交矩阵

矩阵向量的任意向量内积内0，即相互垂直，如果$||u||=1$
，则是向量是一组标准正交基,这样的矩阵叫做标准正交矩阵,正交矩阵有性质:

$$AA^{-1}=AA^{T}=I\  ,\ A^{T}=A^{-1}$$


* 正定矩阵

对任意矩阵M,$x^{T}Mx\geq0$,则M是正定or半正定的(取等号),$y=Mx$表对向量做几何变换,而$x^{T}y$结合上面内积的意义，表示向量x在经过M的变换后，同原x有了夹角，该夹角的度数应该小于等于90°.

* 对称矩阵

这个比较简单，$A=A^{T}$



* 奇异值分解

* 协方差矩阵

描述的是样本的特征(n)之间的相关性，而跟样本数无关，X'*X 为nxmxmxn=nxn维，即只有n的参与。

首先PAC需要以中心化了的$X$的协方差矩阵为计算,意味着$E(x)=0$,PCA的目标是(n维降到k维,kxn x nxk = kxn)

协方差矩阵是一个对称正定矩阵

$$D(x)=\frac{1}{n-1}\sum_{1}^{n}(x-0)^{2}=E(x^2)=tra(W^{t}X*X^{T}W);
\\
st. W^{T}W=I$$

* 特征值分解
* 正定矩阵
* 正交基
